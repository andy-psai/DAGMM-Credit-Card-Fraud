{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements Deep Autoencoding Gaussian Mixture Model (DAGMM)\n",
    "from http://www.cs.ucsb.edu/~bzong/doc/iclr18-dagmm.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses the Kaggle crist card fraud dataset from https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.758743e-12</td>\n",
       "      <td>-8.252298e-13</td>\n",
       "      <td>-9.636929e-13</td>\n",
       "      <td>8.316157e-13</td>\n",
       "      <td>1.591952e-13</td>\n",
       "      <td>4.247354e-13</td>\n",
       "      <td>-3.050180e-13</td>\n",
       "      <td>8.693344e-14</td>\n",
       "      <td>-1.179712e-12</td>\n",
       "      <td>7.094854e-13</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.406543e-13</td>\n",
       "      <td>-5.713163e-13</td>\n",
       "      <td>-9.725303e-13</td>\n",
       "      <td>1.464139e-12</td>\n",
       "      <td>-6.989087e-13</td>\n",
       "      <td>-5.615260e-13</td>\n",
       "      <td>3.332112e-12</td>\n",
       "      <td>-3.518886e-12</td>\n",
       "      <td>2.262609e-13</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>1.088850e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>-2.458826e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>-3.532288e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>-5.354257e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>-3.308395e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>-9.291738e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>-2.652710e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>4.539234e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>-4.471699e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>2.374514e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>1.023621e+02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V1            V2            V3            V4            V5  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   1.758743e-12 -8.252298e-13 -9.636929e-13  8.316157e-13  1.591952e-13   \n",
       "std    1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00  1.380247e+00   \n",
       "min   -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00 -1.137433e+02   \n",
       "25%   -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01 -6.915971e-01   \n",
       "50%    1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02 -5.433583e-02   \n",
       "75%    1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01  6.119264e-01   \n",
       "max    2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01  3.480167e+01   \n",
       "\n",
       "                 V6            V7            V8            V9           V10  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   4.247354e-13 -3.050180e-13  8.693344e-14 -1.179712e-12  7.094854e-13   \n",
       "std    1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00  1.088850e+00   \n",
       "min   -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01 -2.458826e+01   \n",
       "25%   -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01 -5.354257e-01   \n",
       "50%   -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02 -9.291738e-02   \n",
       "75%    3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01  4.539234e-01   \n",
       "max    7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01  2.374514e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...       -3.406543e-13 -5.713163e-13 -9.725303e-13  1.464139e-12   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28        Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -6.989087e-13 -5.615260e-13  3.332112e-12 -3.518886e-12  2.262609e-13   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01  1.000000e+00   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01 -3.532288e-01   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02 -3.308395e-01   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02 -2.652710e-01   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02 -4.471699e-02   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01  1.023621e+02   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# download this Kaggle dataset from:  https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "FILE_PATH = \"c:/Credit_Card_Fraud_Data/\"\n",
    "csv_path = os.path.join(FILE_PATH, \"kaggle_fraud_data.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "# number of features = 30 (features, incl. time) + 1 (label)\n",
    "\n",
    "df = df.drop(['Time'], axis=1)   #drop time feature; number of input features = 29\n",
    "\n",
    "\n",
    "''' \n",
    "# normalize data below \n",
    "cols_to_norm = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10',\n",
    "                'V11','V12','V13','V14','V15','V16','V17','V18','V19','V20', \n",
    "                'V21','V22','V23','V24','V25','V26','V27','V28', 'Amount' ]\n",
    "df.loc[:, cols_to_norm] = (df[cols_to_norm] - df[cols_to_norm].mean()) / df[cols_to_norm].std()\n",
    "\n",
    "'''\n",
    "\n",
    "cols_to_norm = ['Amount' ]\n",
    "\n",
    "df.loc[:, cols_to_norm] = (df[cols_to_norm] - df[cols_to_norm].mean()) / df[cols_to_norm].std()\n",
    "\n",
    "\n",
    "df.head()\n",
    "#df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_data.shape= (284807, 30)\n",
      "shape cc_data_normal (284315, 30)\n",
      "shape cc_data_fraud (492, 30)\n",
      "shape train_total_data_normal (227452, 30)\n",
      "227452\n",
      "shape test_total_data (57355, 30)\n"
     ]
    }
   ],
   "source": [
    "cc_data = df.values    #convert pandas dataframe to numpy array\n",
    "cc_data.shape  # shape -> (284807, 30)\n",
    "print(\"cc_data.shape=\", cc_data.shape)\n",
    "#cc_data_normal = cc_data[cc_data[:,30]==0]   #with 'Time'\n",
    "#cc_data_fraud = cc_data[cc_data[:,30]==1]\n",
    "\n",
    "cc_data_normal = cc_data[cc_data[:,29]==0]    #without 'Time'\n",
    "cc_data_fraud = cc_data[cc_data[:,29]==1]\n",
    "\n",
    "print(\"shape cc_data_normal\", cc_data_normal.shape)    #shape -> (284315, 31)  normal examples\n",
    "print(\"shape cc_data_fraud\", cc_data_fraud.shape)       #shape -> (492, 31)    fraud examples     \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# with Time\n",
    "#train_total_data_normal, test_total_data_normal, y_train_normal, y_test_normal = train_test_split(\n",
    "    #cc_data_normal, cc_data_normal[:,30], test_size=0.2, random_state=21)\n",
    "# without Time\n",
    "train_total_data_normal, test_total_data_normal, y_train_normal, y_test_normal = train_test_split(\n",
    "    cc_data_normal, cc_data_normal[:,29], test_size=0.2, random_state=37)\n",
    "\n",
    "#############################################################################################\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "###############################################################\n",
    "#expects input state shape = (batch_dim,2)  \n",
    "#returns output 'scaled' shape = (batch_dim,2)\n",
    "def process_state(state):\n",
    "    scaled = scaler.transform(state)\n",
    "    return scaled  \n",
    "#################################################################\n",
    "\n",
    "print(\"shape train_total_data_normal\", train_total_data_normal.shape)       \n",
    "total_training_instances = len(train_total_data_normal)\n",
    "print(total_training_instances)\n",
    "\n",
    "test_total_data = np.concatenate((cc_data_fraud, test_total_data_normal), axis=0)\n",
    "\n",
    "print(\"shape test_total_data\", test_total_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_epochs = 50\n",
    "\n",
    "#z_dim = 1  #1   #z compression dimensionality\n",
    "#K_dims = 3  # orig 3\n",
    "#lamda1 = 0.1\n",
    "#lamda2 = 0.01   # 0.01\n",
    "#lr = 0.001   #0.001\n",
    "#batch_size = 1024  # 256, 512, 1024\n",
    "\n",
    "def dagmm(n_epochs=100, hp_drop=0.4, z_dim=1,K_dims=3,lamda1=0.1,lamda2=0.01,lr=0.001,\n",
    "          batch_size=1024, n_hidden1=20, n_hidden2=10, n_hidden3=5, n_layer1=10):\n",
    "    #######################################################################################\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from scipy.optimize import minimize_scalar\n",
    "    tf.reset_default_graph()\n",
    "    training = tf.placeholder_with_default(False, shape=())   \n",
    "\n",
    "    def encoder(x, z_dim, n_hidden1, n_hidden2, n_hidden3, dropout_prob):\n",
    "\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            x_drop = tf.layers.dropout(x, dropout_prob, training=training)\n",
    "            hidden1 = tf.layers.dense(x_drop, n_hidden1, tf.nn.tanh, he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1, dropout_prob, training=training)\n",
    "            hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, tf.nn.tanh, he_init)\n",
    "            hidden2_drop = tf.layers.dropout(hidden2, dropout_prob, training=training)\n",
    "            hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, tf.nn.tanh, he_init)\n",
    "            hidden3_drop = tf.layers.dropout(hidden3, dropout_prob, training=training)\n",
    "            z_comp = tf.layers.dense(hidden3_drop,z_dim,None, he_init)\n",
    "        return z_comp\n",
    "\n",
    "    def decoder(z_comp, x_dim, n_hidden1, n_hidden2, n_hidden3, dropout_prob, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            hidden3 = tf.layers.dense(z_comp, n_hidden3, tf.nn.tanh, he_init)\n",
    "            hidden3_drop = tf.layers.dropout(hidden3, dropout_prob, training=training)\n",
    "            hidden2 = tf.layers.dense(hidden3_drop, n_hidden2, tf.nn.tanh, he_init)\n",
    "            hidden2_drop = tf.layers.dropout(hidden2, dropout_prob, training=training)\n",
    "            hidden1 = tf.layers.dense(hidden2_drop, n_hidden1, tf.nn.tanh, he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1, dropout_prob, training=training)\n",
    "            x_hat_logits = tf.layers.dense(hidden1_drop, x_dim, None, he_init)\n",
    "            #x_hat_sigmoid = tf.sigmoid(x_hat_logits)\n",
    "        return x_hat_logits\n",
    "\n",
    "\n",
    "    def estimation_network(z_concat, K_dims, n_layer1, dropout_prob_paper):\n",
    "        n_softmax=K_dims\n",
    "\n",
    "        with tf.variable_scope(\"estimation\"):\n",
    "            init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            layer1 = tf.layers.dense(z_concat, n_layer1, tf.nn.tanh, he_init)\n",
    "            layer1_drop = tf.layers.dropout(layer1, dropout_prob_paper, training=training)\n",
    "            gamma = tf.layers.dense(layer1_drop, n_softmax, tf.nn.softmax, he_init)\n",
    "            \n",
    "        return gamma\n",
    "\n",
    "    def compression_autoencoder(x, x_dim, z_dim, n_hidden1, n_hidden2, n_hidden3, dropout_prob):\n",
    "\n",
    "        z_comp = encoder(x, z_dim, n_hidden1, n_hidden2, n_hidden3, dropout_prob)\n",
    "        x_hat = decoder(z_comp, x_dim, n_hidden1, n_hidden2, n_hidden3, dropout_prob)\n",
    "\n",
    "        ae_loss = tf.reduce_mean(tf.squared_difference(x,x_hat)) \n",
    "        recon_error = x - x_hat\n",
    "        #cos_similarity and recon_norm are shape = (batch_size,)\n",
    "        recon_error_magnitude = tf.norm(\n",
    "                recon_error, ord='euclidean', axis=1, keep_dims=True) #tensor of shape (batch_size, 1)\n",
    "        x_unit = tf.nn.l2_normalize(x, dim=1)        \n",
    "        x_hat_unit = tf.nn.l2_normalize(x_hat, dim=1)\n",
    "        cos_similarity=tf.reduce_sum(\n",
    "                   tf.multiply(x_unit ,x_hat_unit),axis=1,keep_dims=True) #shape = (batch_size,1)\n",
    "\n",
    "        return z_comp, x_hat, ae_loss, recon_error_magnitude, cos_similarity\n",
    "\n",
    "\n",
    "    def density_parameters(gamma, z):\n",
    "        #gamma expected shape-> (N,K)\n",
    "        #z expected shape-> (N, z_dim)\n",
    "        phi = tf.reduce_mean(gamma, axis=0)   #shape phi= (K,)\n",
    "\n",
    "        gamma_expand = tf.expand_dims(gamma, axis=-1)  #gamma_expand -> (N,K,1)\n",
    "        z_expand =  tf.expand_dims(z, axis=1)          #z_expand -> (N,1,3)\n",
    "        gamma_z = tf.matmul(gamma_expand,z_expand)   #out_shape =(N,K,1)x(N,1,3) = (N,K,3)    for z_dim = 3\n",
    "        mu_numerator = tf.reduce_sum(gamma_z, axis=0)   #shape = (K,3)\n",
    "\n",
    "        mu_denominator = tf.expand_dims(tf.reduce_sum(gamma, axis=0), axis=-1)   #shape = (K,1)\n",
    "        mu = mu_numerator / mu_denominator  #mu shape->(K,3)\n",
    "        #######################################################\n",
    "        #Co-variance:\n",
    "        z_expand = tf.expand_dims(z, axis=1)  #output shape=(N=2,1,3)\n",
    "        mu_expand = tf.expand_dims(mu, axis=0) #output shape=(1,4,3)\n",
    "        z_minus_mu = tf.subtract(z_expand, mu_expand)    #output shape=(N=2,4,3)\n",
    "\n",
    "        z_minus_mu_expand_left = tf.expand_dims(z_minus_mu, axis=2)   #shape=(N=2,4,1,3)\n",
    "        z_minus_mu_expand_right = tf.expand_dims(z_minus_mu, axis=-1)  #shape=(N=2,4,3,1)\n",
    "\n",
    "        z_mu_outer_product = tf.matmul(z_minus_mu_expand_right, z_minus_mu_expand_left)\n",
    "\n",
    "        gamma_double_expand =  tf.expand_dims(tf.expand_dims(gamma, axis=-1), axis=-1)\n",
    "\n",
    "        gamma_x_outer_product = gamma_double_expand * z_mu_outer_product\n",
    "\n",
    "        covariance_numerator = tf.reduce_sum(gamma_x_outer_product, axis=0)   #shape=(K=4,3,3)\n",
    "\n",
    "        g_expand = tf.expand_dims(tf.expand_dims(tf.reduce_sum(gamma, axis=0), axis=-1), axis=-1)\n",
    "\n",
    "        covariance_final = covariance_numerator / g_expand  #entire final covariance term shape=(4,3,3) \n",
    "\n",
    "        return phi, mu, covariance_final\n",
    "\n",
    "    def energy_density(z, phi, mu, covariance, z_dim, K_dims):\n",
    "\n",
    "        z_expand = tf.expand_dims(z, axis=1)  #output shape=(N=2,1,3)\n",
    "        mu_expand = tf.expand_dims(mu, axis=0) #output shape=(1,4,3)\n",
    "        z_minus_mu = tf.subtract(z_expand, mu_expand)    #output shape=(N=2,4,3)\n",
    "\n",
    "        z_minus_mu_expand_left = tf.expand_dims(z_minus_mu, axis=2) #output shape=(N=2,4,1,3)\n",
    "        z_minus_mu_expand_right = tf.expand_dims(z_minus_mu, axis=-1)  #output shape=(N=2,4,3,1)\n",
    "\n",
    "        total_z_dim = z_dim + 2\n",
    "\n",
    "        dummy_matrix = tf.eye(\n",
    "            num_rows = total_z_dim, num_columns = total_z_dim, batch_shape = [K_dims], dtype=tf.float32)\n",
    "        eps = 2.0e-12\n",
    "        eps_matrix = dummy_matrix * eps\n",
    "        covariance = tf.add(covariance, eps_matrix)\n",
    "\n",
    "\n",
    "        covariance_inverse = tf.matrix_inverse(covariance)   #input and output shape = (4,3,3)\n",
    "\n",
    "        cov_inverse_expand = tf.expand_dims(covariance_inverse, axis=0)   #output shape = (1,4,3,3)\n",
    "        cov_inverse_tiled = tf.tile(cov_inverse_expand, [tf.shape(z)[0],1,1,1]) #output shape = (?, 4, 3, 3) \n",
    "\n",
    "        z_mu_x_cov_inverse = tf.matmul(z_minus_mu_expand_left, cov_inverse_tiled)  #output shape=(2, 4, 1, 3)\n",
    "\n",
    "        z_mu_cov_inv_z_mu = tf.matmul(z_mu_x_cov_inverse, z_minus_mu_expand_right) #output shape = (2,4,1,1)\n",
    "\n",
    "        exp_term = tf.exp(-0.5 * z_mu_cov_inv_z_mu)\n",
    "\n",
    "        phi_expand = tf.expand_dims(tf.expand_dims(phi, axis=-1),axis=-1)\n",
    "\n",
    "        phi_exp_product = phi_expand * exp_term\n",
    "        phi_exp_product_squeeze = tf.squeeze(phi_exp_product)\n",
    "\n",
    "        energy_divided_by = tf.expand_dims(\n",
    "                        tf.sqrt(2.0 * 3.14159 * tf.matrix_determinant(covariance)),axis=0) + 2.0e-12\n",
    "\n",
    "        term_to_be_summed = phi_exp_product_squeeze / tf.squeeze(energy_divided_by)\n",
    "\n",
    "        term_inside_log = tf.reduce_sum(term_to_be_summed, axis=1) + 2.0e-12\n",
    "\n",
    "        energy = -1.0 * tf.log(term_inside_log)\n",
    "        final_energy = tf.expand_dims(energy, axis=-1)  #ouptut shape=(?=batch_size,1)\n",
    "\n",
    "        return final_energy\n",
    "    \n",
    "    def test_energy(z, phi_test, mu_test, covariance_test, z_dim, K_dims):\n",
    "\n",
    "        #data_tf = tf.convert_to_tensor(data_np, np.float32)\n",
    "        #convert Numpy numerical value arrays to TF tensor objects\n",
    "        phi = tf.convert_to_tensor(phi_test, np.float32)\n",
    "        mu = tf.convert_to_tensor(mu_test, np.float32)\n",
    "        covariance = tf.convert_to_tensor(covariance_test, np.float32)\n",
    "\n",
    "        total_z_dim = z_dim + 2\n",
    "\n",
    "        dummy_matrix = tf.eye(\n",
    "            num_rows = total_z_dim, num_columns = total_z_dim, batch_shape = [K_dims], dtype=tf.float32)\n",
    "        eps = 2.0e-12\n",
    "        eps_matrix = dummy_matrix * eps\n",
    "        covariance = tf.add(covariance, eps_matrix)\n",
    "\n",
    "\n",
    "        z_expand = tf.expand_dims(z, axis=1)  #output shape=(N=2,1,3)\n",
    "        mu_expand = tf.expand_dims(mu, axis=0) #output shape=(1,4,3)\n",
    "        z_minus_mu = tf.subtract(z_expand, mu_expand)    #output shape=(N=2,4,3)\n",
    "\n",
    "        z_minus_mu_expand_left = tf.expand_dims(z_minus_mu, axis=2)   #output shape=(N=2,4,1,3)\n",
    "        z_minus_mu_expand_right = tf.expand_dims(z_minus_mu, axis=-1)   #output shape=(N=2,4,3,1)\n",
    "\n",
    "\n",
    "        covariance_inverse = tf.matrix_inverse(covariance)   #input and output shape = (4,3,3)\n",
    "\n",
    "        cov_inverse_expand = tf.expand_dims(covariance_inverse, axis=0)   #output shape = (1,4,3,3)\n",
    "        cov_inverse_tiled = tf.tile(cov_inverse_expand, [tf.shape(z)[0],1,1,1]) #output shape = (?, 4, 3, 3)\n",
    "\n",
    "        z_mu_x_cov_inverse = tf.matmul(z_minus_mu_expand_left, cov_inverse_tiled) #output shape=(2, 4, 1, 3)\n",
    "\n",
    "        z_mu_cov_inv_z_mu = tf.matmul(z_mu_x_cov_inverse, z_minus_mu_expand_right) #output shape = (2,4,1,1)\n",
    "\n",
    "        exp_term = tf.exp(-0.5 * z_mu_cov_inv_z_mu)\n",
    "\n",
    "        phi_expand = tf.expand_dims(tf.expand_dims(phi, axis=-1),axis=-1)\n",
    "\n",
    "        phi_exp_product = phi_expand * exp_term\n",
    "        phi_exp_product_squeeze = tf.squeeze(phi_exp_product)\n",
    "\n",
    "        \n",
    "        energy_divided_by = tf.expand_dims(\n",
    "                     tf.sqrt(2.0 * 3.14159 * tf.matrix_determinant(covariance)),axis=0)  + 1.0e-12\n",
    "\n",
    "        term_to_be_summed = phi_exp_product_squeeze / tf.squeeze(energy_divided_by)\n",
    "\n",
    "        term_inside_log = tf.reduce_sum(term_to_be_summed, axis=1) + 1.0e-12\n",
    "\n",
    "        energy = -1.0 * tf.log(term_inside_log)\n",
    "        final_energy_test = tf.expand_dims(energy, axis=-1)  #ouptut shape=(?=batch_size,1)\n",
    "\n",
    "        return final_energy_test\n",
    "\n",
    "    #############################################\n",
    "    from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score\n",
    "    def f1(thresh, energy_array, labels):\n",
    "        prediction = (energy_array>thresh).astype(int)\n",
    "        pred = prediction.reshape(-1)\n",
    "        gt = labels.astype(int)\n",
    "        accuracy = accuracy_score(gt,pred)\n",
    "        precision, recall, f_score, support = prf(gt, pred, average='binary')\n",
    "        return -f_score\n",
    "    #######################################################################################\n",
    "    x_dim  = 29   #with Time dropped there are 29 input features\n",
    "    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    dropout_prob_paper = tf.placeholder(tf.float32, name='dropout_prob_paper')\n",
    "    x = tf.placeholder(tf.float32, shape=[None, x_dim])\n",
    "\n",
    "    z_comp, x_hat, ae_loss, recon_error_magnitude, cos_similarity = compression_autoencoder(\n",
    "                            x, x_dim, z_dim, n_hidden1, n_hidden2, n_hidden3, dropout_prob)\n",
    "\n",
    "    z_concat =  tf.concat([z_comp, recon_error_magnitude, cos_similarity ], axis=1)\n",
    "\n",
    "    gamma = estimation_network(z_concat, K_dims, n_layer1, dropout_prob_paper)\n",
    "\n",
    "    phi, mu, covariance = density_parameters(gamma, z_concat)\n",
    "\n",
    "    energy = energy_density(z_concat, phi, mu, covariance, z_dim, K_dims)\n",
    "\n",
    "    energy_loss = tf.reduce_mean(energy)\n",
    "\n",
    "    indices = [[k_index, j_index, j_index] for k_index in range(K_dims) \n",
    "               for j_index in range(z_dim + 2)]\n",
    "\n",
    "    #indices = [[0,0,0], [0,1,1], [0,2,2], [1,0,0], [1,1,1], \n",
    "                       #[1,2,2], [2,0,0], [2,1,1], [2,2,2], [3,0,0], [3,1,1], [3,2,2]]\n",
    "\n",
    "    cov_loss = tf.gather_nd(covariance, indices)\n",
    "    cov_loss_reciprocal = 1 / cov_loss\n",
    "    P_Sigma_loss = tf.reduce_sum(cov_loss_reciprocal)\n",
    "\n",
    "    total_loss = ae_loss + lamda1*energy_loss + lamda2*P_Sigma_loss \n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "    all_training_vars = [var for var in t_vars if \"encoder\" or \"decoder\" or \"estimation\" in var.name]\n",
    "\n",
    "    training_op = tf.train.AdamOptimizer(lr).minimize(total_loss, var_list=all_training_vars)\n",
    "\n",
    "    number_of_batches = int(total_training_instances / batch_size)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer(), feed_dict={dropout_prob : hp_drop, dropout_prob_paper : hp_drop})\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Random shuffling\n",
    "            np.random.shuffle(train_total_data_normal)\n",
    "            train_data_ = train_total_data_normal[:, 0:x_dim]   \n",
    "            train_label_ = train_total_data_normal[:, x_dim]\n",
    "            #test #################################\n",
    "            test_data = test_total_data[:,0:x_dim]\n",
    "            test_labels = test_total_data[:,x_dim]\n",
    "            #######################################\n",
    "            # Loop over all batches\n",
    "            for i in range(number_of_batches):\n",
    "                # Compute the offset of the current minibatch in the data.\n",
    "                offset = (i * batch_size) % (total_training_instances)\n",
    "                batch_x_input = train_data_[offset:(offset + batch_size), :]\n",
    "\n",
    "          \n",
    "                _, ae_loss_val, energy_loss_val, p_sigma_loss_val, total_loss_val = sess.run(\n",
    "                    (training_op, ae_loss, energy_loss, P_Sigma_loss, total_loss),\n",
    "                    feed_dict={x: batch_x_input, dropout_prob : hp_drop, dropout_prob_paper : hp_drop, training: True})\n",
    "        \n",
    "            #print(\"epoch = \",epoch, \"batch_number = \", i, \n",
    "                  #\"ae_loss=\", ae_loss_val, \"energy_loss=\", energy_loss_val,\n",
    "                  #\"p_sigma_loss_val=\", p_sigma_loss_val, \"total_loss=\", total_loss_val)\n",
    "        \n",
    "            #evaluate energy of total training dataset each training step\n",
    "            energy_loss_val_entire = sess.run((energy_loss), feed_dict={x: train_data_ , \n",
    "                                dropout_prob : 0.0, dropout_prob_paper : 0.0, training: False} )\n",
    "            #print(\"energy loss entire batch=\", energy_loss_val_entire)\n",
    "            '''\n",
    "            Training done;  Start testing\n",
    "            TO DO\n",
    "            1) Calculate fixed phi, mu, and covariance derived on whole training data set\n",
    "            2) Define \"Test_Energy\" function using fixed values above in (1) and taking input points \n",
    "            to give Energy this can calculate energy of 1 point or a batch.\n",
    "            3) Use this to calaculate energy of each data point (normal and anomaly) in test dataset. \n",
    "            If normal/anomaly ratio is 4:1 then define an anomaly as the 80th percentile \n",
    "            energy threshold (find this threshold)\n",
    "            4) Using this threshold and entire test dataset, calcultate predcision-recall curves, F1 score, etc...\n",
    "            #########################################\n",
    "            '''\n",
    "        #1)\n",
    "        phi_test_val, mu_test_val, cov_test_val = sess.run((phi, mu, covariance), \n",
    "                feed_dict={x: train_data_ , dropout_prob : 0.0, dropout_prob_paper : 0.0, training: False} )\n",
    "        \n",
    "        #2) \n",
    "        energy_test = test_energy(z_concat, phi_test_val , mu_test_val, cov_test_val, z_dim, K_dims)\n",
    "        #3)\n",
    "        \n",
    "        energy_test_val = sess.run((energy_test), \n",
    "                                   feed_dict={x: test_data , dropout_prob : 0.0, dropout_prob_paper : 0.0, training: False} )\n",
    "     \n",
    "\n",
    "    result_val = minimize_scalar(f1, args=(energy_test_val,test_labels))\n",
    "    return result_val.fun  #note:  negative of F1-score is returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_f1_score = dagmm(n_epochs=50, z_dim=1,K_dims=3,lamda1=0.1,lamda2=0.01,lr=0.0001,batch_size=1024)\n",
    "print(\"neg F1 score result=\", neg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use scikit optimize (skopt) function gp_minimize to perfrom hyperparamter optimization\n",
    "\n",
    "#import pickle\n",
    "import numpy as np\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \n",
    "    n_epochs, lamda1, lamda2, lr, batch_size, n_hidden1, n_hidden2, n_hidden3, n_layer1 = params\n",
    "    print(\"params:n_epochs, l1, l2, lr, batch_size, n_h1, n_h2, n_h3, n_l1 :\", params)\n",
    "    \n",
    "    neg_f1_score = dagmm(n_epochs=n_epochs, z_dim=1,K_dims=3,lamda1=lamda1,\n",
    "                         lamda2=lamda2,lr=lr,batch_size=batch_size,n_hidden1=n_hidden1, \n",
    "                         n_hidden2=n_hidden2, n_hidden3=n_hidden3, n_layer1=n_layer1)\n",
    "    return neg_f1_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    params = [\n",
    "        (7, 100),\n",
    "        np.logspace(-2, 0, 10),\n",
    "        np.logspace(-3, -1, 10),\n",
    "        np.logspace(-4, -1, 10),\n",
    "        (256, 2000),\n",
    "        (10, 100),\n",
    "        (10, 100),\n",
    "        (3, 10),\n",
    "        (5,30)\n",
    "    ]\n",
    "    res =  gp_minimize(func=main, dimensions=params, n_calls=300, verbose=True)\n",
    "\n",
    "    print(res.x, res.fun)\n",
    "    #pickle.dump(res, open('res.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
